{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a question about your Ontology file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to query ontology file based on user's question.\n",
    "\n",
    "Workflow outline:\n",
    "- Loading graph\n",
    "- Extracting Labels from graph\n",
    "- Find which labels could be related to user's question. NLP sentence transformer technique was utilized for this goal. It detects which labels align closely with given question, and rank related labels.\n",
    "- Find which individuals and datatypes that related labels have.\n",
    "- Provide these individuals and datatypes into the prompt with user's question and let LLM evaluate data and generate answer in natural language.\n",
    "\n",
    "Note: Open source LLM \"ollama\" can be downloaded [here](https://ollama.com)\n",
    "version Llama 3.1 8B version was used in this notebook.\n",
    "\n",
    "Future Work:\n",
    "- Investigate capabilities of GraphRAG\n",
    "- Add initial LLM step that handles user's query and activate GraphRAG functions.\n",
    "- Let LLM write sparql queries if GraphRAG does not work, so our method can be used for generic purposes.\n",
    "- Utilize Langgraph to create complex workflow, and maybe ReAct approach let LLM plan workflow (which functions to use).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import RdfGraph\n",
    "def loadGraph(ontology_source= \"eSCRO_Developing.ttl\"):\n",
    "    graph = RdfGraph(\n",
    "    source_file=ontology_source,\n",
    "    #source_file=\"a3cae519-0ece-48bd-b55e-dd271dd895fc.ttl\",\n",
    "    standard=\"rdf\",\n",
    "    local_copy=\"local_copy.ttl\",\n",
    "    )\n",
    "    graph.load_schema()\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels_from_graph(graph):\n",
    "    query = \"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    SELECT ?class ?label\n",
    "    WHERE {\n",
    "    ?class rdfs:label ?label .\n",
    "    }\n",
    "    \"\"\"\n",
    "    class_labels = graph.query(query)\n",
    "    \n",
    "    # Extract labels from the query result and ensure they are strings\n",
    "    labels = []\n",
    "    for _, label in class_labels:\n",
    "        if label is not None and isinstance(label, str):\n",
    "            labels.append(label.strip())\n",
    "    \n",
    "    print(f\"Extracted {len(labels)} valid labels from the graph.\")\n",
    "    \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# NLP function to find related labels\n",
    "def find_related_labels_byNLP(question, label_list):\n",
    "    query = question\n",
    "    start = time.time()\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')  # Mini model for speed\n",
    "\n",
    "    def find_top_n_similar(query, items, n=3):\n",
    "        query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "        item_embeddings = model.encode(items, convert_to_tensor=True)\n",
    "        \n",
    "        # Calculate cosine similarity between query and all items\n",
    "        similarities = util.pytorch_cos_sim(query_embedding, item_embeddings)[0]\n",
    "        \n",
    "        # Get the indices of the top 'n' most similar items\n",
    "        top_n_indices = similarities.topk(n).indices\n",
    "        \n",
    "        top_n_items = [(items[i], similarities[i].item()) for i in top_n_indices]\n",
    "        return top_n_items\n",
    "\n",
    "    # Find the top N similar labels\n",
    "    top_10_similar = find_top_n_similar(query, label_list, n=1)\n",
    "\n",
    "    end = time.time()\n",
    "    latency = end - start\n",
    "    print(\"Processing time: \", latency)\n",
    "\n",
    "    labels_most_10_similar = []\n",
    "    print(f\"The top 3 most similar items to '{query}' are:\")\n",
    "    for item, score in top_10_similar:\n",
    "        print(f\"'{item}' with a similarity score of {score:.4f}\")\n",
    "        labels_most_10_similar.append(item)\n",
    "        \n",
    "    return labels_most_10_similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the class URI based on a label\n",
    "def find_Individuals_datatypes(graph, top10_labels):\n",
    "    def get_class_from_label(label):\n",
    "        query = f\"\"\"\n",
    "        SELECT ?class WHERE {{\n",
    "            ?class rdfs:label ?label .\n",
    "            FILTER (lcase(str(?label)) = \"{label.lower()}\")\n",
    "        }}\n",
    "        \"\"\"\n",
    "        results = graph.query(query)\n",
    "        if results:\n",
    "            return results[0][\"class\"]\n",
    "        else:\n",
    "            print(f\"No classes found for label: {label}\")\n",
    "            return None\n",
    "\n",
    "    # Function to get individuals\n",
    "    def get_individuals_of_class(class_uri):\n",
    "        query = f\"\"\"\n",
    "        SELECT ?individual WHERE {{\n",
    "            ?individual a <{class_uri}>.\n",
    "        }}\n",
    "        \"\"\"\n",
    "        results = graph.query(query)\n",
    "        individuals = [result[\"individual\"] for result in results]\n",
    "        return individuals\n",
    "    \n",
    "    individuals_list = [] \n",
    "\n",
    "    for label in top10_labels:\n",
    "        class_uri = get_class_from_label(label) \n",
    "        if class_uri:\n",
    "            print(f\"Class URI for label '{label}': {class_uri}\")\n",
    "            individuals = get_individuals_of_class(class_uri)  \n",
    "            if individuals:\n",
    "                individuals_list.extend(individuals)  \n",
    "            print(f\"Individuals of class '{label}': {individuals}\")\n",
    "        else:\n",
    "            print(f\"No class found for label '{label}'\")\n",
    "\n",
    "    # Function to get data properties of an individual\n",
    "    def get_data_properties_of_individual(ind_list):\n",
    "        data_properties_list = []\n",
    "        for ind in ind_list:\n",
    "            query = f\"\"\"\n",
    "            SELECT ?property ?value WHERE {{\n",
    "                <{ind}> ?property ?value .\n",
    "                FILTER isLiteral(?value)\n",
    "            }}\n",
    "            \"\"\"\n",
    "            results = graph.query(query)\n",
    "            data_properties = [(result[\"property\"], result[\"value\"]) for result in results]\n",
    "            data_properties_list.extend(data_properties) \n",
    "        return data_properties_list\n",
    "\n",
    "    # Return the data properties \n",
    "    return get_data_properties_of_individual(individuals_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "def LLM_node(question, data):\n",
    "    def define_label_finding_chain():\n",
    "\n",
    "        #model = define_llm()\n",
    "        model =  Ollama(model=\"llama3.1\")\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "            template = \"\"\"\n",
    "                You are an assistant that has to asnwer user question with provided data.\n",
    "                Evaluate provided data and extract meaning from it to answe user's question.\n",
    "                You must find the answer **only** from the information provided in the \"Provided data\" section and answer the user's question. If the data contains relevant information, extract the relevant part and include it in your answer.\n",
    "\n",
    "                Provided data:\n",
    "                {data}  \n",
    "                End of the data.\n",
    "\n",
    "                User's question:\n",
    "                {question}\n",
    "\n",
    "                Answer:\n",
    "                \"\"\",\n",
    "            input_variables=[\"data\", \"question\"]\n",
    "        )\n",
    "\n",
    "        return prompt | model \n",
    "\n",
    "\n",
    "    chain = define_label_finding_chain()\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"--- LLM ANSWER START --- \")\n",
    "    result = chain.invoke({\"question\": question, \"data\": data})\n",
    "    print(result)\n",
    "    print(\"--- LLM ANSWER END --- \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final step: RUN\n",
    "\n",
    "I am going to run all of these functions above in sequence at the momemnt.\n",
    "\n",
    "In future, I will utilize LangGraph code to call functions in more complex way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 2728 valid labels from the graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atacankuralavgoren/myenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time:  14.563163995742798\n",
      "The top 3 most similar items to 'What is the maximum height limitation in the traffic?' are:\n",
      "'maximum height without special approval' with a similarity score of 0.6253\n",
      "Most related labels: ['maximum height without special approval']\n",
      "Class URI for label 'maximum height without special approval': http://quantecton.com/kb/eSCRO#MaximumHeightWithoutSpecialApproval\n",
      "Individuals of class 'maximum height without special approval': [rdflib.term.URIRef('http://quantecton.com/kb/eSCRO#TransportMaxHeight')]\n",
      "\n",
      "\n",
      "\n",
      "--- LLM ANSWER START --- \n",
      "Based on the provided data, it appears that there is a property \"hasSimpleExpressionValue\" associated with a URI \"https://spec.industrialontologies.org/ontology/core/Core/\" and a value of \"4.5m\".\n",
      "\n",
      "To answer the user's question about the maximum height limitation in traffic, I would extract the relevant part of this information:\n",
      "\n",
      "\"...a value of '4.5m'\"\n",
      "\n",
      "This suggests that the maximum height limitation in traffic is 4.5 meters.\n",
      "\n",
      "Answer:\n",
      "The maximum height limitation in traffic is 4.5 meters.\n",
      "--- LLM ANSWER END --- \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ## ASK Your Question\n",
    "    Question = \"What is the maximum height limitation in the traffic?\"\n",
    "\n",
    "    #Load the graph\n",
    "    graph = loadGraph(ontology_source= \"eSCRO_Developing.ttl\")\n",
    "    \n",
    "    # Extract Labels from the ontology\n",
    "    label_list = extract_labels_from_graph(graph)\n",
    "    \n",
    "    # Find most 10 related labels with the question\n",
    "    top_10_most_related = find_related_labels_byNLP(Question, label_list)\n",
    "\n",
    "    print(\"Most related labels:\", top_10_most_related)\n",
    "\n",
    "    # Find individuals  of these classes\n",
    "    individuals = find_Individuals_datatypes(graph, top_10_most_related)\n",
    "\n",
    "    # Run LLM to answer question in natural language\n",
    "    result = LLM_node(Question, individuals)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langgraph.graph import Graph\n",
    "\n",
    "# workflow = Graph()\n",
    "\n",
    "# workflow.add_node(\"label\", extract_labels_from_graph)\n",
    "# workflow.add_node(\"nlp\", find_related_labels_byNLP)\n",
    "\n",
    "# workflow.add_edge('label','nlp')\n",
    "\n",
    "# workflow.add_node(\"Individuals\", find_Individuals_datatypes)\n",
    "\n",
    "# workflow.add_edge('nlp','Individuals')\n",
    "\n",
    "# workflow.add_node(\"llm\", LLM_node)\n",
    "\n",
    "# workflow.add_edge('Individuals','llm')\n",
    "\n",
    "# workflow.set_entry_point('label')\n",
    "# workflow.set_finish_point('llm')\n",
    "\n",
    "# app = workflow.compile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
